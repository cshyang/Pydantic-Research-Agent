{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow\n",
    "\n",
    "Input topic -> Topic Expansion (list) \\\n",
    "Topic Expansion -> Research on relevant ropics (dict) \\\n",
    "Research on relevant topics -> Generate Outline (str) \\\n",
    "Generate Outline -> Generate Article (str) \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Expansion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI in Recruitment', 'Impact of AI on Labor Markets', 'Ethical Considerations of AI in the Workplace']\n"
     ]
    }
   ],
   "source": [
    "class RelatedTopics(BaseModel):\n",
    "    topics: List[str] = Field(description=\"A list of related topics with maximum of 3.\")\n",
    "\n",
    "\n",
    "# Create agent\n",
    "topic_explorer_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    result_type=RelatedTopics,\n",
    "    system_prompt=(\n",
    "        \"You are an experienced researcher. Given a research topic, return a list of relevant topics to research on.\"\n",
    "        \"Avoid returning topics that are too broad or too specific. \"\n",
    "        \"The list of topics will then use to research on wikipedia to find relevant information. \"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "async def gen_related_topics(topic: str):\n",
    "    related_topics = await topic_explorer_agent.run(topic)\n",
    "    # related_topics_str = \", \".join(related_topics.data.topics)\n",
    "    return related_topics.data.topics\n",
    "\n",
    "\n",
    "topic = \"AI Agents in Workforce.\"\n",
    "\n",
    "related_topics = await gen_related_topics(topic)\n",
    "print(related_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Outline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_retriever = WikipediaRetriever(top_k_results=1)\n",
    "\n",
    "wiki_retriever_results = await wiki_retriever.abatch(related_topics)\n",
    "wiki_docs = wiki_retriever_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'title': 'Applications of artificial intelligence', 'summary': 'Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programes emulate perception and understanding, and are designed to adapt to new information and new situations. Machine learning has been used for various scientific and commercial purposes including language translation, image recognition, decision-making, credit scoring, and e-commerce.', 'source': 'https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence'}, page_content=\"Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programes emulate perception and understanding, and are designed to adapt to new information and new situations. Machine learning has been used for various scientific and commercial purposes including language translation, image recognition, decision-making, credit scoring, and e-commerce.\\n\\n\\n== Internet and e-commerce ==\\n\\n\\n=== Web feeds and posts ===\\nMachine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds. Various types of social media analysis also make use of machine learning and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.\\nAI has been used to customize shopping options and personalize offers. Online gambling companies have used AI for targeting gamblers.\\n\\n\\n=== Virtual assistants and search ===\\n\\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.\\nBing Chat has used artificial intelligence as part of its search engine.\\n\\n\\n=== Spam filtering ===\\n\\nMachine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements. Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails. These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.\\n\\n\\n=== Language translation ===\\n\\nSpeech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another. \\nAI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator. Additionally, research and development are in progress to decode and conduct animal communication.\\nMeaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.\\n\\n\\n=== Facial recognition and image labeling ===\\n\\nAI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.\\nImage labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people. Facebook's DeepFace identifies human faces in digital images.\\n\\n\\n== Games and entertainment ==\\n\\nGames have been a major application of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson), Go (AlphaGo), poker (Pluribus and Cepheus), E-sports (StarCraft), and general game playing (AlphaZero and MuZero).\\nKuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool. Character.ai is another example of a chatbot being used for recreation.\\n\\n\\n== Economic and social challe\")],\n",
       " [Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nSince its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet\\'s automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\n\\n\\n=== Academic artificial intelligence ===\\nThe academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince its inception, the field of machine learning used both discriminative models and generative models, to mode')],\n",
       " [Document(metadata={'title': 'Ethics of artificial intelligence', 'summary': 'The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \\nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.', 'source': 'https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence'}, page_content='The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \\nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\\n\\n\\n== Machine ethics ==\\n\\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI\\'s decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit â€“ or if they end up developing human \\'weaknesses\\' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\\n\\n\\n=== Robot ethics ===\\n\\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\\n\\n\\n=== Ethical principles ===\\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, tr')]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#Local AI Adoption: Transforming Communities through Artificial Intelligence\n",
       "\n",
       "## Understanding Local AI Adoption\n",
       "\n",
       "Artificial intelligence (AI) adoption at the local level refers to the integration and utilization of AI technologies by local governments, businesses, and communities to improve public services, enhance decision-making, and foster economic growth. With the rise of AI capabilities, local entities are increasingly exploring how to leverage AI's potential to address community-specific challenges, streamline operations, and better serve constituents. This article provides an overview of local AI adoption, its benefits, challenges, and community-driven initiatives that are shaping its landscape.\n",
       "\n",
       "\n",
       "\n",
       "## The Benefits of Local AI Adoption\n",
       "\n",
       "AI offers transformative benefits for local communities across various sectors. Key advantages include:\n",
       "\n",
       "1. **Enhanced Decision-Making**: AI enables the analysis of vast amounts of data, providing valuable insights for informed policy making.\n",
       "2. **Improved Service Delivery**: Local governments can use AI to automate routine tasks and enhance service responsiveness, ultimately leading to better citizen experiences.\n",
       "3. **Community Development**: AI technologies drive community development by fostering innovation, improving resource allocation, and increasing operational efficiencies.\n",
       "4. **Economic Growth**: By facilitating new business models and optimizing systems, AI adoption can stimulate job creation and economic development in local economies.\n",
       "5. **Personalized Community Engagement**: AI helps tailor communication with residents, allowing local authorities to better meet specific needs and preferences.\n",
       "\n",
       "### Insights into Community Development\n",
       "\n",
       " AI's ability to analyze and interpret data offers profound insights into community trends, needs, and preferences. For instance, local governments can monitor traffic patterns or evaluate public health data to implement changes that enhance overall community well-being.\n",
       "\n",
       "## Challenges to Local AI Adoption\n",
       "\n",
       "Despite the numerous advantages, local AI adoption is not without challenges. Key hurdles include:\n",
       "\n",
       "1. **Lack of Awareness**: Many local governments and organizations face a knowledge gap regarding AI technologies and their potential benefits.\n",
       "2. **Budget Constraints**: Limited financial resources can hinder investments in AI infrastructure and expertise, curtailing the ability to adopt these technologies.\n",
       "3. **Regulatory Barriers**: Complex and evolving AI regulations at the national and local levels can impede the adoption process, leaving organizations wary of compliance risks.\n",
       "4. **Talent Shortages**: The demand for skilled professionals in AI is high, but many local governments struggle to recruit and retain experts.\n",
       "5. **Public Perception and Acceptance**: There may be concerns regarding privacy, bias, and the ethical implications of AI, impacting community buy-in and support for AI initiatives.\n",
       "\n",
       "### Navigating Regulatory Challenges\n",
       "\n",
       " Local regulations can significantly affect AI implementation. Authorities must consider compliance with both local and federal laws governing data usage, privacy protection, and algorithmic accountability. Establishing clear guidelines can enable smooth AI adoption while addressing community concerns.\n",
       "\n",
       "## Community-Driven AI Initiatives\n",
       "\n",
       "Community-driven initiatives in AI involve local residents in the development and deployment of AI technologies. This collaborative approach ensures that the voices of those affected are included in decision-making processes, leading to more tailored solutions that genuinely reflect community needs. Examples of community-driven AI initiatives include:\n",
       "\n",
       "1. **Participatory Design Workshops**: Engaging community members in the design process of AI solutions to meet local needs and preferences.\n",
       "2. **Open-Source Development**: Encouraging collaboration among local developers and stakeholders to share knowledge, tools, and data, thereby cultivating a supportive innovation ecosystem.\n",
       "3. **Predictive Analytics for Public Safety**: Harnessing AI to analyze historical crime data and help law enforcement agencies allocate resources effectively, with community input on desired approaches to policing.\n",
       "\n",
       "### Empowering Local Voices\n",
       "\n",
       " By empowering communities to have a say in AI projects, local governments can foster trust and minimize resistance. Initiatives like town hall meetings, surveys, and focus groups allow for two-way communication between local authorities and citizens, ensuring AI solutions align with community values.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "As local entities navigate the evolving landscape of AI adoption, balancing benefits with challenges is vital. Embracing community participation and fostering a supportive environment will be crucial for the successful implementation of AI technologies. By navigating the regulatory complexities, investing in education and training, and prioritizing ethical considerations, local governments can harness AI's full potential to enhance services, drive innovation, and contribute to the overall prosperity of their communities.\n",
       "\n",
       "\n",
       "\n",
       "## Future Trends in Local AI Adoption\n",
       "\n",
       "As we look ahead, several trends are likely to shape the future of AI adoption at the local level:\n",
       "\n",
       "1. **Increased Collaboration**: Local governments will likely collaborate with tech companies and research institutions to leverage expertise and resources for AI initiatives.\n",
       "2. **Focus on Ethical AI**: There will be greater emphasis on developing ethical AI guidelines to ensure responsible use of technology and to build public trust.\n",
       "3. **Smart City Initiatives**: More local governments will integrate AI into their smart city projects to optimize urban infrastructure and public services.\n",
       "4. **Training Programs**: Expect the emergence of training programs aimed at equipping local officials and staff with the necessary skills for effective AI integration.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(article.data.as_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research on the topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'related_topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     35\u001b[0m tavily_search \u001b[38;5;241m=\u001b[39m TavilySearch()\n\u001b[0;32m---> 36\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tavily_search\u001b[38;5;241m.\u001b[39mabatch_search(\u001b[43mrelated_topics\u001b[49m)\n\u001b[1;32m     37\u001b[0m search_results_str \u001b[38;5;241m=\u001b[39m TavilySearch\u001b[38;5;241m.\u001b[39mresults_to_str(search_results)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(search_results_str)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'related_topics' is not defined"
     ]
    }
   ],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "\n",
    "class TavilySearch:\n",
    "    def __init__(self, max_results: int = 3):\n",
    "        TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "        self.tavily_client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "        self.max_results = max_results\n",
    "\n",
    "    async def asearch(self, query: str) -> dict:\n",
    "        results = {}\n",
    "        response = await self.tavily_client.search(query, max_results=self.max_results)\n",
    "        search_results = response[\"results\"]\n",
    "        for r in search_results:\n",
    "            source = {r[\"url\"]: r[\"content\"]}\n",
    "            results.update(source)\n",
    "        return results\n",
    "\n",
    "    async def abatch_search(self, queries: list[str]) -> dict:\n",
    "        search_results = {}\n",
    "        for query in queries:\n",
    "            results = await self.asearch(query)\n",
    "            search_results.update(results)\n",
    "        return search_results\n",
    "\n",
    "    @staticmethod\n",
    "    def results_to_str(search_results: dict, max_len: int = 50_000) -> str:\n",
    "        return \"\\n\\n##########\\n\".join(\n",
    "            f\"URL:\\n {url}\\n\\nContent:\\n{content}\"\n",
    "            for url, content in search_results.items()\n",
    "        )[:max_len]\n",
    "\n",
    "\n",
    "# Usage\n",
    "tavily_search = TavilySearch()\n",
    "search_results = await tavily_search.abatch_search(related_topics)\n",
    "search_results_str = TavilySearch.results_to_str(search_results)\n",
    "\n",
    "print(search_results_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Outline\n",
    "class Subsection(BaseModel):\n",
    "    title: str = Field(description=\"The title of the subsection.\")\n",
    "    content: str = Field(description=\"The content of the subsection.\")\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    title: str = Field(description=\"The title of the section.\")\n",
    "    content: str = Field(description=\"The content of the section.\")\n",
    "    subtitles: List[Subsection] = Field(description=\"A list of subsections.\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self):\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {s.title}\\n\\n {s.content}\" for s in self.subtitles\n",
    "        )\n",
    "        return f\"## {self.title}\\n\\n{self.content}\\n\\n{subsections}\"\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    title: str = Field(description=\"The title of the article.\")\n",
    "    sections: List[Section]\n",
    "\n",
    "    @property\n",
    "    def as_str(self):\n",
    "        sections = \"\\n\\n\".join(f\"{s.as_str}\" for s in self.sections)\n",
    "        return f\"# {self.title}\\n\\n{sections}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Article\n",
    "gen_article_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    result_type=Outline,\n",
    "    deps_type=Optional[Outline],\n",
    "    system_prompt=(\n",
    "        \"You are an experienced researcher.\"\n",
    "        \"Based on the resouces gathered so far, generate an article based on the outline.\"\n",
    "        \"Use only the information provided gathered anf cite the url of the source for each section.\"\n",
    "        \"Make sure the article is informative and cohesive.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "example_article = await gen_article_agent.run(\n",
    "    search_results_str,\n",
    "    result_type=Outline,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining everything together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Tool\n",
    "\n",
    "gen_article_agent = Agent(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    result_type=Outline,\n",
    "    deps_type=Optional[Outline],\n",
    "    system_prompt=(\n",
    "        \"You are an experienced researcher, given a research topic, use gen_related_topics to find more relevant topics.\"\n",
    "        \"Use tavily_search.abatch_search tool to gather relevant information with the relevant topics.\"\n",
    "        \"Based on the resouces gathered, generate an article based on the outline.\"\n",
    "        \"Use only the information provided gathered anf cite the url of the source for each section.\"\n",
    "        \"Make sure the article is informative and cohesive.\"\n",
    "    ),\n",
    "    tools=[Tool(tavily_search.abatch_search), Tool(gen_related_topics)],\n",
    ")\n",
    "\n",
    "article = await gen_article_agent.run(\"local AI adoption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(article.data.as_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Based on the research results, update the initial outline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(example_article.data.as_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article.all_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
